"""
Celeryå¼‚æ­¥ä»»åŠ¡å®šä¹‰
å¤„ç†ä¿é™©æ–‡æ¡£OCRç»“æœçš„ä¸ƒä¸ªæ ¸å¿ƒä»»åŠ¡ï¼ˆæŒ‰æ‰§è¡Œé¡ºåºï¼‰ï¼š
0. OCRè¯†åˆ« (ocr_document_task) - æ–°å¢
1. æå–è¡¨æ ¼æºä»£ç  (extract_tablecontent_task)
2. æå–åŸºæœ¬ä¿¡æ¯ (extract_basic_info_task)
3. æå–è¡¨æ ¼æ¦‚è¦ (extract_tablesummary_task)
4. æå–å¹´åº¦ä»·å€¼è¡¨ (extract_table_task)
5. æå–æ— å¿§é€‰é€€ä¿ä»·å€¼è¡¨ (extract_wellness_table_task)
6. æå–è®¡åˆ’ä¹¦æ¦‚è¦ (extract_summary_task)
"""
import logging
import time
import re
import json
import os
import requests
from celery import shared_task
from django.utils import timezone
from openai import OpenAI
from .models import PlanDocument
from .deepseek_service import extract_plan_data_from_text, analyze_insurance_table, extract_plan_summary, extract_table_summary

logger = logging.getLogger(__name__)


@shared_task(bind=True, max_retries=2, default_retry_delay=60)
def ocr_document_task(self, document_id):
    """
    æ­¥éª¤0ï¼šOCRè¯†åˆ«æ–‡æ¡£
    è°ƒç”¨PaddleLayout APIè¯†åˆ«PDFæ–‡æ¡£ï¼Œæå–markdownæ ¼å¼æ–‡æœ¬

    Args:
        document_id: PlanDocumentçš„ID

    Returns:
        dict: {'success': bool, 'error': str (if failed)}
    """
    try:
        logger.info("=" * 80)
        logger.info(f"ğŸ“„ Celeryä»»åŠ¡å¼€å§‹ - æ­¥éª¤0/7: OCRè¯†åˆ«æ–‡æ¡£ - æ–‡æ¡£ID: {document_id}")
        logger.info(f"   é‡è¯•æ¬¡æ•°: {self.request.retries}/{self.max_retries}")

        # åŠ è½½æ–‡æ¡£
        try:
            doc = PlanDocument.objects.get(id=document_id)
        except PlanDocument.DoesNotExist:
            error_msg = f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨"
            logger.error(error_msg)
            return {'success': False, 'error': error_msg}

        # æ›´æ–°çŠ¶æ€
        doc.processing_stage = 'ocr_processing'
        doc.last_processed_at = timezone.now()
        doc.save(update_fields=['processing_stage', 'last_processed_at'])

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not doc.file_path or not os.path.exists(doc.file_path.path):
            error_msg = "PDFæ–‡ä»¶ä¸å­˜åœ¨"
            logger.error(f"âŒ {error_msg}: {doc.file_path}")
            doc.processing_stage = 'error'
            doc.error_message = error_msg
            doc.status = 'failed'
            doc.save()
            return {'success': False, 'error': error_msg}

        # è°ƒç”¨PaddleLayout API
        logger.info(f"ğŸ“¤ å¼€å§‹è°ƒç”¨PaddleLayout OCR: {doc.file_path.path}")

        try:
            with open(doc.file_path.path, 'rb') as pdf_file:
                files = {
                    'file': (doc.file_name, pdf_file, 'application/pdf')
                }
                data = {
                    'format': 'markdown'
                }
                headers = {
                    'X-API-Key': '0dbe66d87befa7a9d5d7c1bdbc631a9b7dc5ce88be9a20e41c26790060802647'
                }

                response = requests.post(
                    'http://localhost:5003/api/paddle-layout/pdf',
                    files=files,
                    data=data,
                    headers=headers,
                    timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
                )

            if response.status_code == 200:
                ocr_content = response.text
                logger.info(f"âœ… OCRè¯†åˆ«æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(ocr_content)}")

                if not ocr_content or not ocr_content.strip():
                    raise Exception("OCRè¿”å›å†…å®¹ä¸ºç©º")

                # ä¿å­˜OCRå†…å®¹åˆ°æ•°æ®åº“
                doc.content = ocr_content
                doc.processing_stage = 'ocr_completed'
                doc.last_processed_at = timezone.now()
                doc.save(update_fields=['content', 'processing_stage', 'last_processed_at'])

                logger.info("âœ… æ­¥éª¤0å®Œæˆ: OCRè¯†åˆ«æˆåŠŸ")

                # è‡ªåŠ¨è§¦å‘ä¸‹ä¸€ä¸ªä»»åŠ¡ï¼šæå–è¡¨æ ¼æºä»£ç 
                extract_tablecontent_task.apply_async(args=[document_id], countdown=2)

                return {'success': True}

            else:
                error_msg = f"PaddleLayout APIé”™è¯¯: {response.status_code} - {response.text[:200]}"
                raise Exception(error_msg)

        except requests.exceptions.Timeout:
            error_msg = "OCRè¯·æ±‚è¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

        except requests.exceptions.RequestException as e:
            error_msg = f"OCRè¯·æ±‚å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

    except Exception as e:
        error_msg = f"OCRè¯†åˆ«å¤±è´¥: {str(e)}"
        logger.error(error_msg)

        # é‡è¯•æœºåˆ¶
        if self.request.retries < self.max_retries:
            logger.warning(f"â³ å°†åœ¨60ç§’åé‡è¯• ({self.request.retries + 1}/{self.max_retries})")
            raise self.retry(exc=Exception(error_msg))

        # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°åï¼Œæ ‡è®°å¤±è´¥
        logger.error(f"âŒ å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒOCRè¯†åˆ«å¤±è´¥")
        doc.processing_stage = 'error'
        doc.error_message = f"OCRè¯†åˆ«å¤±è´¥ï¼ˆå·²é‡è¯•{self.max_retries}æ¬¡ï¼‰: {error_msg}"
        doc.status = 'failed'
        doc.save()

        return {'success': False, 'error': error_msg}


@shared_task(bind=True, max_retries=2, default_retry_delay=60)
def extract_basic_info_task(self, document_id):
    """
    æ­¥éª¤2ï¼šæå–åŸºæœ¬ä¿¡æ¯
    ä»OCRæ–‡æœ¬ä¸­æå–å—ä¿äººã€ä¿é™©äº§å“ã€ä¿é¢ã€ä¿è´¹ç­‰æ ¸å¿ƒä¿¡æ¯

    Args:
        document_id: PlanDocumentçš„ID

    Returns:
        dict: {'success': bool, 'error': str (if failed)}
    """
    try:
        logger.info("=" * 80)
        logger.info(f"ğŸ“‹ Celeryä»»åŠ¡å¼€å§‹ - æ­¥éª¤2/6: æå–åŸºæœ¬ä¿¡æ¯ - æ–‡æ¡£ID: {document_id}")
        logger.info(f"   é‡è¯•æ¬¡æ•°: {self.request.retries}/{self.max_retries}")

        # åŠ è½½æ–‡æ¡£
        try:
            doc = PlanDocument.objects.get(id=document_id)
        except PlanDocument.DoesNotExist:
            error_msg = f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨"
            logger.error(error_msg)
            return {'success': False, 'error': error_msg}

        # æ£€æŸ¥OCRæ˜¯å¦æˆåŠŸï¼ˆå‰ç½®æ¡ä»¶ï¼‰
        if doc.status == 'failed' or doc.processing_stage == 'error':
            error_msg = f"OCRè¯†åˆ«å¤±è´¥ï¼Œè·³è¿‡åç»­ä»»åŠ¡: {doc.error_message}"
            logger.error(f"âŒ {error_msg}")
            return {'success': False, 'error': error_msg}

        # æ›´æ–°çŠ¶æ€
        doc.processing_stage = 'extracting_basic_info'
        doc.last_processed_at = timezone.now()
        doc.save(update_fields=['processing_stage', 'last_processed_at'])

        if not doc.content:
            error_msg = "æ–‡æ¡£å†…å®¹ä¸ºç©º"
            logger.error(f"âŒ {error_msg}")
            # é‡‡ç”¨é™çº§ç­–ç•¥ï¼šè‡³å°‘ä¿ç•™OCRå†…å®¹
            doc.processing_stage = 'basic_info_completed'
            doc.error_message = f"åŸºæœ¬ä¿¡æ¯æå–å¤±è´¥: {error_msg}ï¼Œä½†OCRå†…å®¹å·²ä¿å­˜"
            doc.status = 'completed'  # é™çº§ä¸ºéƒ¨åˆ†å®Œæˆ
            doc.save()
            # ç»§ç»­æ‰§è¡Œåç»­ä»»åŠ¡
            logger.info("âš ï¸ é™çº§ç­–ç•¥ï¼šè·³è¿‡åŸºæœ¬ä¿¡æ¯æå–ï¼Œç»§ç»­åç»­æ­¥éª¤")
            extract_table_task.apply_async(args=[document_id], countdown=2)
            return {'success': False, 'error': error_msg, 'degraded': True}

        # è°ƒç”¨DeepSeekæå–åŸºæœ¬ä¿¡æ¯
        basic_result = extract_plan_data_from_text(doc.content)

        if not basic_result or not basic_result.get('success'):
            error_msg = f"åŸºæœ¬ä¿¡æ¯æå–å¤±è´¥: {basic_result.get('error') if basic_result else 'æœªçŸ¥é”™è¯¯'}"
            logger.error(error_msg)
            # é‡è¯•æœºåˆ¶
            if self.request.retries < self.max_retries:
                logger.warning(f"â³ å°†åœ¨60ç§’åé‡è¯• ({self.request.retries + 1}/{self.max_retries})")
                raise self.retry(exc=Exception(error_msg))

            # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°åï¼Œé‡‡ç”¨é™çº§ç­–ç•¥
            logger.error(f"âŒ å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå¯ç”¨é™çº§ç­–ç•¥")
            doc.processing_stage = 'basic_info_completed'
            doc.error_message = f"åŸºæœ¬ä¿¡æ¯æå–å¤±è´¥ï¼ˆå·²é‡è¯•{self.max_retries}æ¬¡ï¼‰ï¼Œä½†OCRå†…å®¹å·²ä¿å­˜"
            doc.status = 'completed'  # é™çº§ä¸ºéƒ¨åˆ†å®Œæˆ
            doc.save()
            # ç»§ç»­æ‰§è¡Œåç»­ä»»åŠ¡
            logger.info("âš ï¸ é™çº§ç­–ç•¥ï¼šè·³è¿‡åŸºæœ¬ä¿¡æ¯æå–ï¼Œç»§ç»­åç»­æ­¥éª¤")
            extract_table_task.apply_async(args=[document_id], countdown=2)
            return {'success': False, 'error': error_msg, 'degraded': True}

        basic_data = basic_result.get('data', {})

        # æ›´æ–°æ•°æ®åº“å­—æ®µ
        doc.insured_name = basic_data.get('insured_name', '')

        # å¹´é¾„è½¬æ¢
        try:
            age = basic_data.get('insured_age')
            doc.insured_age = int(age) if age else None
        except (ValueError, TypeError):
            doc.insured_age = None

        doc.insured_gender = basic_data.get('insured_gender', '')
        doc.insurance_product = basic_data.get('insurance_product', '')
        doc.insurance_company = basic_data.get('insurance_company', '')

        # ä¿é¢è½¬æ¢
        try:
            amount = basic_data.get('sum_assured')
            doc.sum_assured = int(float(amount)) if amount else None
        except (ValueError, TypeError):
            doc.sum_assured = None

        # å¹´ç¼´ä¿è´¹è½¬æ¢
        try:
            premium = basic_data.get('annual_premium')
            doc.annual_premium = int(float(premium)) if premium else None
        except (ValueError, TypeError):
            doc.annual_premium = None

        # ç¼´è´¹å¹´æ•°è½¬æ¢
        try:
            payment_years = basic_data.get('payment_years')
            if payment_years:
                numbers = re.findall(r'\d+', str(payment_years))
                doc.payment_years = int(numbers[0]) if numbers else None
            else:
                doc.payment_years = None
        except (ValueError, TypeError, IndexError):
            doc.payment_years = None

        doc.insurance_period = basic_data.get('insurance_period', '')
        doc.extracted_data = basic_data

        # æ›´æ–°é˜¶æ®µä¸ºå®Œæˆ
        doc.processing_stage = 'basic_info_completed'
        doc.last_processed_at = timezone.now()
        doc.save()

        logger.info("âœ… æ­¥éª¤2å®Œæˆ: åŸºæœ¬ä¿¡æ¯æå–æˆåŠŸ")
        logger.info(f"   - å—ä¿äºº: {doc.insured_name}")
        logger.info(f"   - ä¿é™©äº§å“: {doc.insurance_product}")

        # è‡ªåŠ¨è§¦å‘ä¸‹ä¸€ä¸ªä»»åŠ¡ï¼šæå–è¡¨æ ¼æ¦‚è¦
        extract_tablesummary_task.apply_async(args=[document_id], countdown=2)

        return {'success': True}

    except Exception as e:
        error_msg = f"æå–åŸºæœ¬ä¿¡æ¯æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
        logger.error(error_msg)
        import traceback
        logger.error(traceback.format_exc())

        # é‡è¯•æœºåˆ¶
        if self.request.retries < self.max_retries:
            logger.warning(f"â³ å‘ç”Ÿå¼‚å¸¸ï¼Œå°†åœ¨60ç§’åé‡è¯• ({self.request.retries + 1}/{self.max_retries})")
            raise self.retry(exc=e)

        # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°åï¼Œé‡‡ç”¨é™çº§ç­–ç•¥
        logger.error(f"âŒ å¼‚å¸¸å¤„ç†ï¼šå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå¯ç”¨é™çº§ç­–ç•¥")
        try:
            doc = PlanDocument.objects.get(id=document_id)
            doc.processing_stage = 'basic_info_completed'
            doc.error_message = f"åŸºæœ¬ä¿¡æ¯æå–å¼‚å¸¸ï¼ˆå·²é‡è¯•{self.max_retries}æ¬¡ï¼‰: {error_msg}ï¼Œä½†OCRå†…å®¹å·²ä¿å­˜"
            doc.status = 'completed'  # é™çº§ä¸ºéƒ¨åˆ†å®Œæˆ
            doc.save()
            # ç»§ç»­æ‰§è¡Œåç»­ä»»åŠ¡
            logger.info("âš ï¸ é™çº§ç­–ç•¥ï¼šè·³è¿‡åŸºæœ¬ä¿¡æ¯æå–ï¼Œç»§ç»­åç»­æ­¥éª¤")
            extract_table_task.apply_async(args=[document_id], countdown=2)
        except Exception as save_error:
            logger.error(f"ä¿å­˜é™çº§çŠ¶æ€æ—¶å‡ºé”™: {save_error}")

        return {'success': False, 'error': error_msg, 'degraded': True}


@shared_task(bind=True, max_retries=2, default_retry_delay=60)
def extract_table_task(self, document_id):
    """
    æ­¥éª¤4ï¼šæå–é€€ä¿ä»·å€¼è¡¨ (table1å­—æ®µ)
    è°ƒç”¨ content_editor_views.process_surrender_value_table æ ¸å¿ƒå‡½æ•°

    Args:
        document_id: PlanDocumentçš„ID

    Returns:
        dict: {'success': bool, 'error': str (if failed)}
    """
    from api.content_editor_views import process_surrender_value_table

    try:
        logger.info("=" * 80)
        logger.info(f"ğŸ“Š Celeryä»»åŠ¡å¼€å§‹ - æ­¥éª¤4/6: æå–é€€ä¿ä»·å€¼è¡¨ - æ–‡æ¡£ID: {document_id}")

        # æ›´æ–°çŠ¶æ€ä¸ºæ­£åœ¨æå–
        try:
            doc = PlanDocument.objects.get(id=document_id)
        except PlanDocument.DoesNotExist:
            error_msg = f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨"
            logger.error(error_msg)
            return {'success': False, 'error': error_msg}

        # æ£€æŸ¥OCRæ˜¯å¦æˆåŠŸï¼ˆå‰ç½®æ¡ä»¶ï¼‰
        if doc.status == 'failed' or doc.processing_stage == 'error':
            error_msg = f"OCRè¯†åˆ«å¤±è´¥ï¼Œè·³è¿‡åç»­ä»»åŠ¡: {doc.error_message}"
            logger.error(f"âŒ {error_msg}")
            return {'success': False, 'error': error_msg}

        try:
            doc.processing_stage = 'extracting_table'
            doc.last_processed_at = timezone.now()
            doc.save(update_fields=['processing_stage', 'last_processed_at'])
        except Exception as e:
            error_msg = f"æ›´æ–°çŠ¶æ€å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return {'success': False, 'error': error_msg}

        # è°ƒç”¨æ ¸å¿ƒå‡½æ•°å¤„ç†
        result = process_surrender_value_table(document_id)

        # æ›´æ–°çŠ¶æ€ä¸ºå·²å®Œæˆ
        doc.processing_stage = 'table_completed'
        doc.last_processed_at = timezone.now()
        doc.save(update_fields=['processing_stage', 'last_processed_at'])

        if result['success']:
            logger.info(f"âœ… æ­¥éª¤4å®Œæˆ: é€€ä¿ä»·å€¼è¡¨æå–æˆåŠŸ")
        else:
            logger.warning(f"âš ï¸ æ­¥éª¤4å®Œæˆ: {result.get('message', result.get('error', 'æœªæ‰¾åˆ°é€€ä¿ä»·å€¼è¡¨'))}")

        # è‡ªåŠ¨è§¦å‘ä¸‹ä¸€ä¸ªä»»åŠ¡ï¼šæå–æ— å¿§é€‰é€€ä¿ä»·å€¼è¡¨
        extract_wellness_table_task.apply_async(args=[document_id], countdown=2)

        return result

    except Exception as e:
        logger.error(f"âŒ æå–é€€ä¿ä»·å€¼è¡¨æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        logger.error(traceback.format_exc())

        # å³ä½¿å¤±è´¥ä¹Ÿç»§ç»­ä¸‹ä¸€ä¸ªä»»åŠ¡
        try:
            doc = PlanDocument.objects.get(id=document_id)
            doc.table1 = ''
            doc.processing_stage = 'table_completed'
            doc.save()
            extract_wellness_table_task.apply_async(args=[document_id], countdown=2)
        except:
            pass

        # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°åç»§ç»­
        if self.request.retries >= self.max_retries:
            return {'success': False, 'error': str(e)}

        raise self.retry(exc=e)

@shared_task(bind=True, max_retries=2, default_retry_delay=60)
def extract_wellness_table_task(self, document_id):
    """
    æ­¥éª¤5ï¼šæå–æ— å¿§é€‰é€€ä¿ä»·å€¼è¡¨ (table2å­—æ®µ)
    è°ƒç”¨ content_editor_views.process_wellness_table æ ¸å¿ƒå‡½æ•°

    Args:
        document_id: PlanDocumentçš„ID

    Returns:
        dict: {'success': bool, 'error': str (if failed)}
    """
    from api.content_editor_views import process_wellness_table

    try:
        logger.info("=" * 80)
        logger.info(f"ğŸ’° Celeryä»»åŠ¡å¼€å§‹ - æ­¥éª¤5/6: æå–æ— å¿§é€‰é€€ä¿ä»·å€¼è¡¨ - æ–‡æ¡£ID: {document_id}")

        # æ›´æ–°çŠ¶æ€ä¸ºæ­£åœ¨æå–
        try:
            doc = PlanDocument.objects.get(id=document_id)
        except PlanDocument.DoesNotExist:
            error_msg = f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨"
            logger.error(error_msg)
            return {'success': False, 'error': error_msg}

        # æ£€æŸ¥OCRæ˜¯å¦æˆåŠŸï¼ˆå‰ç½®æ¡ä»¶ï¼‰
        if doc.status == 'failed' or doc.processing_stage == 'error':
            error_msg = f"OCRè¯†åˆ«å¤±è´¥ï¼Œè·³è¿‡åç»­ä»»åŠ¡: {doc.error_message}"
            logger.error(f"âŒ {error_msg}")
            return {'success': False, 'error': error_msg}

        doc.processing_stage = 'extracting_wellness_table'
        doc.last_processed_at = timezone.now()
        doc.save(update_fields=['processing_stage', 'last_processed_at'])

        # è°ƒç”¨æ ¸å¿ƒå‡½æ•°å¤„ç†
        result = process_wellness_table(document_id)

        # æ›´æ–°çŠ¶æ€ä¸ºå·²å®Œæˆ
        doc.processing_stage = 'wellness_table_completed'
        doc.last_processed_at = timezone.now()
        doc.save(update_fields=['processing_stage', 'last_processed_at'])

        if result['success']:
            logger.info(f"âœ… æ­¥éª¤5å®Œæˆ: æ— å¿§é€‰é€€ä¿ä»·å€¼è¡¨æå–æˆåŠŸ")
        else:
            logger.warning(f"âš ï¸ æ­¥éª¤5å®Œæˆ: {result.get('message', result.get('error', 'æœªæ‰¾åˆ°æ— å¿§é€‰é€€ä¿ä»·å€¼è¡¨'))}")

        # è‡ªåŠ¨è§¦å‘ä¸‹ä¸€ä¸ªä»»åŠ¡ï¼šæå–è®¡åˆ’ä¹¦æ¦‚è¦
        extract_summary_task.apply_async(args=[document_id], countdown=2)

        return result

    except Exception as e:
        logger.error(f"âŒ æå–æ— å¿§é€‰é€€ä¿ä»·å€¼è¡¨æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        logger.error(traceback.format_exc())

        # å³ä½¿å¤±è´¥ä¹Ÿç»§ç»­ä¸‹ä¸€ä¸ªä»»åŠ¡
        try:
            doc = PlanDocument.objects.get(id=document_id)
            doc.table2 = ''
            doc.processing_stage = 'wellness_table_completed'
            doc.save()
            extract_summary_task.apply_async(args=[document_id], countdown=2)
        except:
            pass

        # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°åç»§ç»­
        if self.request.retries >= self.max_retries:
            return {'success': False, 'error': str(e)}

        raise self.retry(exc=e)

@shared_task(bind=True, max_retries=2, default_retry_delay=60)
def extract_summary_task(self, document_id):
    """
    æ­¥éª¤6ï¼šæå–è®¡åˆ’ä¹¦æ¦‚è¦ï¼ˆä½¿ç”¨å¹´åº¦ä»·å€¼è¡¨æ•°æ®ï¼‰
    ä»OCRæ–‡æœ¬ä¸­æå–è®¡åˆ’ä¹¦çš„æ•´ä½“æ¦‚è¦ã€å…³é”®ç‚¹ã€é‡è¦æ—¥æœŸç­‰ä¿¡æ¯

    Args:
        document_id: PlanDocumentçš„ID

    Returns:
        dict: {'success': bool, 'error': str (if failed)}
    """
    try:
        logger.info("=" * 80)
        logger.info(f"ğŸ“ Celeryä»»åŠ¡å¼€å§‹ - æ­¥éª¤6/6: æå–è®¡åˆ’ä¹¦æ¦‚è¦ - æ–‡æ¡£ID: {document_id}")

        # åŠ è½½æ–‡æ¡£
        try:
            doc = PlanDocument.objects.get(id=document_id)
        except PlanDocument.DoesNotExist:
            error_msg = f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨"
            logger.error(error_msg)
            return {'success': False, 'error': error_msg}

        # æ£€æŸ¥OCRæ˜¯å¦æˆåŠŸï¼ˆå‰ç½®æ¡ä»¶ï¼‰
        if doc.status == 'failed' or doc.processing_stage == 'error':
            error_msg = f"OCRè¯†åˆ«å¤±è´¥ï¼Œè·³è¿‡åç»­ä»»åŠ¡: {doc.error_message}"
            logger.error(f"âŒ {error_msg}")
            return {'success': False, 'error': error_msg}

        # æ›´æ–°çŠ¶æ€
        doc.processing_stage = 'extracting_summary'
        doc.last_processed_at = timezone.now()
        doc.save(update_fields=['processing_stage', 'last_processed_at'])

        if not doc.content:
            error_msg = "æ–‡æ¡£å†…å®¹ä¸ºç©º"
            logger.warning(error_msg)
            # æ¦‚è¦å¤±è´¥ä¹Ÿä¸æ˜¯è‡´å‘½é”™è¯¯
            doc.processing_stage = 'all_completed'
            doc.status = 'completed'
            doc.save()
            return {'success': False, 'error': error_msg}

        # è§£ætable1æ•°æ®ç”¨äºè®¡ç®—é‡Œç¨‹ç¢‘
        table1_data = None
        if doc.table1:
            try:
                import json
                table1_data = json.loads(doc.table1) if isinstance(doc.table1, str) else doc.table1
            except (json.JSONDecodeError, TypeError):
                logger.warning("âš ï¸ table1æ•°æ®è§£æå¤±è´¥ï¼Œå°†ä¸åŒ…å«é‡Œç¨‹ç¢‘ä¿¡æ¯")
                table1_data = None

        # è§£ætable2æ•°æ®ç”¨äºæ”¶å…¥è§„åˆ’
        table2_data = None
        if doc.table2:
            try:
                import json
                table2_data = json.loads(doc.table2) if isinstance(doc.table2, str) else doc.table2
            except (json.JSONDecodeError, TypeError):
                logger.warning("âš ï¸ table2æ•°æ®è§£æå¤±è´¥ï¼Œå°†ä¸åŒ…å«æ”¶å…¥è§„åˆ’")
                table2_data = None

        # è°ƒç”¨DeepSeekæå–æ¦‚è¦ï¼ˆè¿”å›Markdownæ–‡æœ¬ï¼Œä¼ å…¥table1å’Œtable2æ•°æ®ï¼‰
        summary_data = extract_plan_summary(
            doc.content,
            table1_data,
            doc.annual_premium,
            doc.payment_years,
            table2_data,
            doc.insured_age
        )

        # æ£€æŸ¥æ˜¯å¦æˆåŠŸæå–ï¼ˆç©ºå­—ç¬¦ä¸²è¡¨ç¤ºå¤±è´¥ï¼‰
        if not summary_data or len(summary_data.strip()) == 0:
            error_msg = "è®¡åˆ’ä¹¦æ¦‚è¦æå–å¤±è´¥"
            logger.warning(f"{error_msg}ï¼ˆä½†åŸºæœ¬ä¿¡æ¯å’Œå¹´åº¦è¡¨å·²ä¿å­˜ï¼‰")
            # æ¦‚è¦å¤±è´¥ä¹Ÿä¸æ˜¯è‡´å‘½é”™è¯¯ï¼Œä¿å­˜ç©ºå­—ç¬¦ä¸²
            doc.summary = ''
            doc.processing_stage = 'all_completed'
            doc.status = 'completed'
            doc.save()
            return {'success': False, 'error': error_msg}

        # æ›´æ–°æ•°æ®åº“ - ç›´æ¥ä¿å­˜Markdownæ–‡æœ¬
        doc.summary = summary_data
        doc.processing_stage = 'all_completed'
        doc.status = 'completed'
        doc.last_processed_at = timezone.now()
        doc.save()

        logger.info(f"ğŸ“ Summaryå·²ä¿å­˜åˆ°æ•°æ®åº“ï¼Œé•¿åº¦: {len(summary_data)} å­—ç¬¦")
        logger.info("âœ… æ­¥éª¤6å®Œæˆ: è®¡åˆ’ä¹¦æ¦‚è¦æå–æˆåŠŸ")
        logger.info(f"   - æ¦‚è¦é•¿åº¦: {len(summary_data)} å­—ç¬¦")
        logger.info(f"   - æ¦‚è¦é¢„è§ˆ: {summary_data[:100]}...")
        logger.info("=" * 80)
        logger.info(f"ğŸ‰ æ–‡æ¡£ {document_id} æ‰€æœ‰6ä¸ªä»»åŠ¡å¤„ç†å®Œæˆï¼")
        logger.info("=" * 80)

        return {'success': True}

    except Exception as e:
        error_msg = f"æå–è®¡åˆ’ä¹¦æ¦‚è¦æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
        logger.warning(f"{error_msg}ï¼ˆä½†åŸºæœ¬ä¿¡æ¯å’Œå¹´åº¦è¡¨å·²ä¿å­˜ï¼‰")
        import traceback
        logger.error(traceback.format_exc())

        # æ¦‚è¦å¤±è´¥ä¹Ÿä¸æ˜¯è‡´å‘½é”™è¯¯
        try:
            doc = PlanDocument.objects.get(id=document_id)
            doc.processing_stage = 'all_completed'
            doc.status = 'completed'
            doc.save()
        except:
            pass

        return {'success': False, 'error': error_msg}


@shared_task(bind=True, max_retries=2, default_retry_delay=60)
def extract_tablecontent_task(self, document_id):
    """
    æ­¥éª¤1ï¼šæå–è¡¨æ ¼æºä»£ç 
    ä»OCRæ–‡æœ¬ä¸­æå–æ‰€æœ‰<table>æ ‡ç­¾çš„æºä»£ç å¹¶ä¿å­˜

    Args:
        document_id: PlanDocumentçš„ID

    Returns:
        dict: {'success': bool, 'error': str (if failed)}
    """
    try:
        logger.info("=" * 80)
        logger.info(f"ğŸ“Š Celeryä»»åŠ¡å¼€å§‹ - æ­¥éª¤1/6: æå–è¡¨æ ¼æºä»£ç  - æ–‡æ¡£ID: {document_id}")

        # åŠ è½½æ–‡æ¡£
        try:
            doc = PlanDocument.objects.get(id=document_id)
        except PlanDocument.DoesNotExist:
            error_msg = f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨"
            logger.error(error_msg)
            return {'success': False, 'error': error_msg}

        # æ£€æŸ¥OCRæ˜¯å¦æˆåŠŸï¼ˆå‰ç½®æ¡ä»¶ï¼‰
        if doc.status == 'failed' or doc.processing_stage == 'error':
            error_msg = f"OCRè¯†åˆ«å¤±è´¥ï¼Œè·³è¿‡åç»­ä»»åŠ¡: {doc.error_message}"
            logger.error(f"âŒ {error_msg}")
            return {'success': False, 'error': error_msg}

        if not doc.content:
            error_msg = "OCRå†…å®¹ä¸ºç©ºï¼Œæ— æ³•ç»§ç»­å¤„ç†"
            logger.error(f"âŒ {error_msg}")
            doc.processing_stage = 'error'
            doc.status = 'failed'
            doc.error_message = error_msg
            doc.save()
            return {'success': False, 'error': error_msg}

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰<table>æ ‡ç­¾
        import re
        table_regex = re.compile(r'<table[^>]*>([\s\S]*?)</table>', re.IGNORECASE)
        matches = table_regex.findall(doc.content)

        if matches:
            # æå–å®Œæ•´çš„<table>...</table>
            tables = []
            for match_obj in table_regex.finditer(doc.content):
                tables.append(match_obj.group(0))

            # åˆå¹¶æ‰€æœ‰è¡¨æ ¼ï¼Œä½¿ç”¨åŒæ¢è¡Œåˆ†éš”
            tablecontent = '\n\n'.join(tables)
            doc.tablecontent = tablecontent

            logger.info(f"âœ… æå–åˆ° {len(tables)} ä¸ªè¡¨æ ¼")
            logger.info(f"   - æ€»é•¿åº¦: {len(tablecontent)} å­—ç¬¦")

            doc.save(update_fields=['tablecontent'])

            logger.info("âœ… æ­¥éª¤1å®Œæˆ: è¡¨æ ¼æºä»£ç æå–æˆåŠŸ")
            logger.info("=" * 80)

            # è‡ªåŠ¨è§¦å‘ä¸‹ä¸€ä¸ªä»»åŠ¡ï¼šæå–åŸºæœ¬ä¿¡æ¯
            extract_basic_info_task.apply_async(args=[document_id], countdown=2)

            return {'success': True}
        else:
            # ç†è®ºä¸Šä¸åº”è¯¥åˆ°è¿™é‡Œï¼Œå› ä¸ºwebhookå·²ç»é¢„æ£€æŸ¥è¿‡äº†
            # ä½†ä¿ç•™ä½œä¸ºäºŒæ¬¡ä¿æŠ¤
            error_msg = "æœªæ£€æµ‹åˆ°è¡¨æ ¼å…ƒç´ ï¼ˆä»»åŠ¡é“¾äºŒæ¬¡æ£€æŸ¥ï¼‰"
            logger.warning(f"âš ï¸ {error_msg}")
            doc.tablecontent = ''
            doc.save(update_fields=['tablecontent'])
            # ç»§ç»­åç»­ä»»åŠ¡ï¼Œå› ä¸ºå¯èƒ½æ˜¯è¾¹ç¼˜æƒ…å†µ
            extract_basic_info_task.apply_async(args=[document_id], countdown=2)
            return {'success': True}

    except Exception as e:
        error_msg = f"æå–è¡¨æ ¼æºä»£ç æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        import traceback
        logger.error(traceback.format_exc())

        try:
            doc = PlanDocument.objects.get(id=document_id)
            doc.tablecontent = ''
            doc.processing_stage = 'error'
            doc.status = 'failed'
            doc.error_message = error_msg
            doc.save()
        except Exception as save_error:
            logger.error(f"ä¿å­˜é”™è¯¯çŠ¶æ€å¤±è´¥: {save_error}")

        logger.info("â›” ä»»åŠ¡é“¾å·²ç»ˆæ­¢ï¼šè¡¨æ ¼æå–å¼‚å¸¸")
        return {'success': False, 'error': error_msg, 'terminate': True}


@shared_task(bind=True, max_retries=2, default_retry_delay=60)
def extract_tablesummary_task(self, document_id):
    """
    æ­¥éª¤3ï¼šæå–è¡¨æ ¼æ¦‚è¦
    ä½¿ç”¨DeepSeek APIåˆ†æOCRå†…å®¹ä¸­çš„è¡¨æ ¼ç»“æ„
    æå–è¡¨æ ¼åç§°ã€è¡Œæ•°ã€å­—æ®µä¿¡æ¯

    Args:
        document_id: PlanDocumentçš„ID

    Returns:
        dict: {'success': bool, 'error': str (if failed)}
    """
    try:
        logger.info("=" * 80)
        logger.info(f"ğŸ“‹ Celeryä»»åŠ¡å¼€å§‹ - æ­¥éª¤3/6: æå–è¡¨æ ¼æ¦‚è¦ - æ–‡æ¡£ID: {document_id}")

        # åŠ è½½æ–‡æ¡£
        try:
            doc = PlanDocument.objects.get(id=document_id)
        except PlanDocument.DoesNotExist:
            error_msg = f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨"
            logger.error(error_msg)
            return {'success': False, 'error': error_msg}

        # æ£€æŸ¥OCRæ˜¯å¦æˆåŠŸï¼ˆå‰ç½®æ¡ä»¶ï¼‰
        if doc.status == 'failed' or doc.processing_stage == 'error':
            error_msg = f"OCRè¯†åˆ«å¤±è´¥ï¼Œè·³è¿‡åç»­ä»»åŠ¡: {doc.error_message}"
            logger.error(f"âŒ {error_msg}")
            return {'success': False, 'error': error_msg}

        if not doc.content:
            error_msg = "æ–‡æ¡£å†…å®¹ä¸ºç©º"
            logger.warning(error_msg)
            # æ¦‚è¦å¤±è´¥ä¹Ÿä¸æ˜¯è‡´å‘½é”™è¯¯
            doc.tablesummary = ''
            doc.save(update_fields=['tablesummary'])
            # ç»§ç»­ä¸‹ä¸€ä¸ªä»»åŠ¡
            extract_table_task.apply_async(args=[document_id], countdown=2)
            return {'success': False, 'error': error_msg}

        # è·å–DeepSeek APIå¯†é’¥
        api_key = os.getenv('DEEPSEEK_API_KEY')
        if not api_key:
            error_msg = 'DEEPSEEK_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®'
            logger.error(f'âŒ {error_msg}')
            # æ¦‚è¦å¤±è´¥ä¹Ÿä¸æ˜¯è‡´å‘½é”™è¯¯
            doc.tablesummary = ''
            doc.save(update_fields=['tablesummary'])
            # ç»§ç»­ä¸‹ä¸€ä¸ªä»»åŠ¡
            extract_table_task.apply_async(args=[document_id], countdown=2)
            return {'success': False, 'error': error_msg}

        # åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
        client = OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com"
        )

        # æ„å»ºæç¤ºè¯ï¼ˆä¸content_editor_views.pyä¸­å®Œå…¨ä¸€è‡´ï¼‰
        prompt = f"""ä»¥ä¿å•å¹´åº¦ç»ˆç»“ä¸ºåæ ‡ï¼Œåˆ†æä»¥ä¸‹ä¿é™©è®¡åˆ’ä¹¦ä¸­çš„æ‰€æœ‰è¡¨æ ¼ã€‚

è¦æ±‚ï¼š
1. è¯†åˆ«æ‰€æœ‰ä»¥"ä¿å•å¹´åº¦ç»ˆç»“"ä¸ºåæ ‡çš„è¡¨æ ¼
2. æœ‰äº›è¡¨æ ¼å¯èƒ½è·¨åº¦å¥½å‡ ä¸ªé¡µé¢ï¼Œä½†åªç®—ä¸€å¼ è¡¨ï¼Œè¯·å®Œæ•´è¯†åˆ«
3. å¯¹æ¯ä¸ªè¡¨æ ¼æå–ï¼šè¡¨è¯¦ç»†åç§°ã€è¡Œæ•°ã€åŸºæœ¬å­—æ®µ

åªè¾“å‡ºç»“æœï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šè¯´æ˜ã€‚

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
1.
è¡¨åï¼šè©³ç´°èªªæ˜ - é€€ä¿åƒ¹å€¼ (åªæ ¹æ“šåŸºæœ¬è¨ˆåŠƒè¨ˆç®—)
è¡Œæ•°ï¼š100è¡Œ
åŸºæœ¬å­—æ®µï¼šä¿å•å¹´åº¦ç»ˆç»“,ç¼´ä»˜ä¿è´¹æ€»é¢,é€€ä¿ä»·å€¼(ä¿è¯é‡‘é¢(ä¿è¯ç°é‡‘ä»·å€¼),éä¿è­‰é‡‘é¡(ç»­æœŸçº¢åˆ©),æ€»é¢),ç´¯ç©å·²æ”¯ä»˜éä¿è­‰å…¥æ¯+ç¸½é€€ä¿åƒ¹å€¼

2.
è¡¨åï¼šèº«æ•…è³ å„Ÿ
è¡Œæ•°ï¼š50è¡Œ
åŸºæœ¬å­—æ®µï¼šä¿å•å¹´åº¦ç»ˆç»“,èº«æ•…èµ”å¿(ä¿è¯é‡‘é¢,éä¿è¯é‡‘é¢,æ€»é¢)

è®¡åˆ’ä¹¦å†…å®¹ï¼š
{doc.content[:120000]}

è¯·ç›´æ¥è¿”å›åˆ†æç»“æœï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°ã€‚"""

        logger.info("â³ å¼€å§‹è°ƒç”¨ DeepSeek API åˆ†æè¡¨æ ¼ç»“æ„")
        logger.info(f"   OCRå†…å®¹é•¿åº¦: {len(doc.content)} å­—ç¬¦")
        logger.info(f"   ä½¿ç”¨å†…å®¹é•¿åº¦: {min(len(doc.content), 120000)} å­—ç¬¦")

        # è°ƒç”¨DeepSeek API
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿é™©æ–‡æ¡£åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿è¯†åˆ«å’Œåˆ†æè¡¨æ ¼ç»“æ„ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.3,
            max_tokens=2000
        )

        # æå–ç»“æœ
        content = response.choices[0].message.content.strip()
        logger.info(f"ğŸ“¦ DeepSeek APIè¿”å›ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")

        # æ¸…ç†å¯èƒ½çš„ä»£ç å—æ ‡è®°
        if content.startswith('```'):
            lines = content.split('\n')
            # ç§»é™¤ç¬¬ä¸€è¡Œï¼ˆ```ï¼‰å’Œæœ€åä¸€è¡Œï¼ˆ```ï¼‰
            if len(lines) > 2 and lines[-1].strip() == '```':
                content = '\n'.join(lines[1:-1])
            elif len(lines) > 1:
                content = '\n'.join(lines[1:])

        # æœ€ç»ˆæ£€æŸ¥
        if not content or len(content.strip()) == 0:
            error_msg = "è¡¨æ ¼æ¦‚è¦æå–å¤±è´¥ï¼Œè¿”å›å†…å®¹ä¸ºç©º"
            logger.warning(f"{error_msg}ï¼ˆä½†å…¶ä»–æ•°æ®å·²ä¿å­˜ï¼‰")
            # æ¦‚è¦å¤±è´¥ä¹Ÿä¸æ˜¯è‡´å‘½é”™è¯¯ï¼Œä¿å­˜ç©ºå­—ç¬¦ä¸²
            doc.tablesummary = ''
            doc.save(update_fields=['tablesummary'])
            # ç»§ç»­ä¸‹ä¸€ä¸ªä»»åŠ¡
            extract_table_task.apply_async(args=[document_id], countdown=2)
            return {'success': False, 'error': error_msg}

        # æ›´æ–°æ•°æ®åº“ - ç›´æ¥ä¿å­˜æ–‡æœ¬
        doc.tablesummary = content
        doc.last_processed_at = timezone.now()
        doc.save(update_fields=['tablesummary', 'last_processed_at'])

        logger.info(f"ğŸ“‹ è¡¨æ ¼æ¦‚è¦å·²ä¿å­˜åˆ°æ•°æ®åº“ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")

        # æ›´æ–°ä¼šå‘˜ä½¿ç”¨ç»Ÿè®¡
        if doc.user:
            from .models import Membership
            try:
                membership = Membership.objects.get(user=doc.user)
                membership.documents_created += 1
                membership.save()
                logger.info(f"ğŸ“Š ä¼šå‘˜å·²åˆ›å»ºè®¡åˆ’ä¹¦æ•°: {membership.documents_created}")
            except Membership.DoesNotExist:
                pass

        logger.info("âœ… æ­¥éª¤3å®Œæˆ: è¡¨æ ¼æ¦‚è¦æå–æˆåŠŸ")
        logger.info(f"   - æ¦‚è¦é•¿åº¦: {len(content)} å­—ç¬¦")
        logger.info(f"   - æ¦‚è¦é¢„è§ˆ: {content[:200]}...")

        # è‡ªåŠ¨è§¦å‘ä¸‹ä¸€ä¸ªä»»åŠ¡ï¼šæå–å¹´åº¦ä»·å€¼è¡¨
        extract_table_task.apply_async(args=[document_id], countdown=2)

        return {'success': True}

    except Exception as e:
        error_msg = f"æå–è¡¨æ ¼æ¦‚è¦æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
        logger.warning(f"{error_msg}ï¼ˆä½†å…¶ä»–æ•°æ®å·²ä¿å­˜ï¼‰")
        import traceback
        logger.error(traceback.format_exc())

        # æ¦‚è¦å¤±è´¥ä¹Ÿä¸æ˜¯è‡´å‘½é”™è¯¯
        try:
            doc = PlanDocument.objects.get(id=document_id)
            doc.tablesummary = ''
            doc.save(update_fields=['tablesummary'])
            # ç»§ç»­ä¸‹ä¸€ä¸ªä»»åŠ¡
            extract_table_task.apply_async(args=[document_id], countdown=2)
        except:
            pass

        return {'success': False, 'error': error_msg}


@shared_task
def process_document_pipeline(document_id):
    """
    å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµæ°´çº¿å…¥å£
    æŒ‰é¡ºåºè§¦å‘å…­ä¸ªä»»åŠ¡ï¼šè¡¨æ ¼æºä»£ç  -> åŸºæœ¬ä¿¡æ¯ -> è¡¨æ ¼æ¦‚è¦ -> å¹´åº¦ä»·å€¼è¡¨ -> æ— å¿§é€‰é€€ä¿ä»·å€¼è¡¨ -> è®¡åˆ’ä¹¦æ¦‚è¦

    Args:
        document_id: PlanDocumentçš„ID
    """
    logger.info(f"ğŸš€ å¯åŠ¨æ–‡æ¡£å¤„ç†æµæ°´çº¿ï¼ˆ6ä¸ªä»»åŠ¡ï¼‰- æ–‡æ¡£ID: {document_id}")

    # è§¦å‘ç¬¬ä¸€ä¸ªä»»åŠ¡ï¼šæå–è¡¨æ ¼æºä»£ç ï¼Œåç»­ä»»åŠ¡ä¼šè‡ªåŠ¨é“¾å¼è§¦å‘
    extract_tablecontent_task.apply_async(args=[document_id])

    return {'status': 'pipeline_started', 'document_id': document_id}
